{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36cc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def make_chroma_db(documents):\n",
    "    # Chunking\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    # 벡터 저장소 만들기\n",
    "    db = Chroma.from_documents(docs, OpenAIEmbeddings(), persist_directory=\"chroma_db\")\n",
    "    return db\n",
    "\n",
    "def get_top5_docs_from_db(query):\n",
    "    db = Chroma(persist_directory=\"chroma_db\", embedding_function=OpenAIEmbeddings())\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 5}) # 상위 5개만 추출하도록 설정\n",
    "\n",
    "    return retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API 키 설정 (환경변수 또는 직접 입력)\n",
    "client = OpenAI()\n",
    "\n",
    "# 🔍 GPT를 사용해 요약 생성\n",
    "def summarize_with_gpt(content: str, file_path: str, max_chars: int = 1500) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    다음은 '{file_path}'라는 파일의 코드입니다. \n",
    "    이 파일의 목적이 무엇인지, 어떤 기능이 있고 어떤 문제를 해결하는지 간단히 요약해 주세요. \n",
    "    \\n\\n```python\\n{content[:max_chars]}\\n```\\n\\n요약:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",  # 또는 gpt-3.5-turbo\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPT 요약 실패 ({file_path}): {e}\")\n",
    "        return \"요약 실패\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee5bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[설치] python-dotenv 패키지를 설치합니다...\n",
      "[설치] python-dotenv 패키지 설치 완료\n",
      "[Git] Git이 설치되어 있습니다: C:\\Program Files\\Git\\cmd\\git.EXE\n",
      "\n",
      "[정보] 저장소 소유자: hwangchahae\n",
      "[정보] 저장소 이름: coding_test_study\n",
      "\n",
      "[정보] 전체 저장소 내용을 가져오는 중...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\USER\\Desktop\\GitHub\\3rd_project\")  # chahae 폴더의 상위 폴더\n",
    "\n",
    "from chahae.github_repo_viewer import main\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "documents = main(os.environ.get(\"GITHUB_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96354467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\2972093822.py:12: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  db = Chroma.from_documents(docs, OpenAIEmbeddings(), persist_directory=\"chroma_db\")\n"
     ]
    }
   ],
   "source": [
    "chroma_db = make_chroma_db(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b39d088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\2972093822.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=\"chroma_db\", embedding_function=OpenAIEmbeddings())\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\2972093822.py:19: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_name': 'README.md', 'source': 'week_10/1697_숨바꼭질/README.md'}, page_content='# 숨바꼭질 문제 풀이\\n\\n백준 문제 번호: 1697'),\n",
       " Document(metadata={'source': 'week_8/15683_감시/README.md', 'file_name': 'README.md'}, page_content='# 감시 문제 풀이\\n\\n백준 문제 번호: 15683'),\n",
       " Document(metadata={'source': 'README.md', 'file_name': 'README.md'}, page_content='# 알고리즘 풀이 (2025.03.27 ~)\\n\\n## 📌 이번 주 문제'),\n",
       " Document(metadata={'source': 'README.md', 'file_name': 'README.md'}, page_content='## 📌 스터디 방식\\n1. 풀 플랫폼은 [백준](https://www.acmicpc.net/)을 사용한다.\\n2. 알고리즘 스터디는 정해진 문제를 푼다.\\n3. 각자 문제 풀이 후 스터디 모임 때 코드 리뷰를 진행한다.\\n4. 스터디 진행 방식 참고: [코딩테스트 진행 방식](https://dev-dain.tistory.com/155)\\n\\n## 🏆 문제 풀이 기록'),\n",
       " Document(metadata={'file_name': 'README.md', 'source': 'week_10/1012_유기농_배추/README.md'}, page_content='# 유기농 배추 문제 풀이\\n\\n백준 문제 번호: 1012')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"큐 관련 문제 코드 찾아줘\"\n",
    "results = get_top5_docs_from_db(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0890605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'README.md', 'source': 'week_10/1697_숨바꼭질/README.md'}\n"
     ]
    }
   ],
   "source": [
    "print(results[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403b4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\1783291465.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\1783291465.py:9: LangChainDeprecationWarning: This function is deprecated. Refer to this guide on retrieval and question answering with sources: https://python.langchain.com/docs/how_to/qa_sources/\n",
      "See also the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "  qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\1783291465.py:10: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 설명 결과:\n",
      " I'm sorry, but the provided contents do not contain any information about the code for a problem related to \"큐\".\n",
      "SOURCES:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "\n",
    "# 4. LLM 준비\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# 5. 문서를 기반으로 설명 생성 (Chain 사용)\n",
    "qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "# 6. 출력\n",
    "print(\"📝 설명 결과:\\n\", result[\"output_text\"])\n",
    "# print(\"\\n📚 참고된 문서 정보:\\n\", result[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac38b91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided contents do not contain any information about the code for a problem related to \"큐\".\n",
      "SOURCES:\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc581aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class MemoryRAGRetriever:\n",
    "    def __init__(self, collection: Chroma):\n",
    "        self.vectorstore = collection\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "        self.retriever_with_memory = create_history_aware_retriever(\n",
    "            llm=self.llm,\n",
    "            retriever=self.retriever,\n",
    "            prompt=PromptTemplate.from_template(\"\"\"\n",
    "다음은 사용자와의 최근 대화 일부입니다:\n",
    "{chat_history}\n",
    "\n",
    "사용자의 질문: {input}\n",
    "\n",
    "위 질문에 가장 관련된 내용을 찾기 위한 쿼리를 만들어주세요.\n",
    "\"\"\")\n",
    "        )\n",
    "\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            retriever=self.retriever_with_memory,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        self.chat_history = ChatMessageHistory()\n",
    "\n",
    "    def query(self, user_query: str) -> str:\n",
    "        # 최근 3쌍만 유지 (user + ai = 6개)\n",
    "        recent_history = self.chat_history.messages[-6:]\n",
    "        \n",
    "        result = self.qa_chain.invoke({\n",
    "            \"input\": user_query,\n",
    "            \"chat_history\": recent_history\n",
    "        })\n",
    "\n",
    "        self.chat_history.add_user_message(user_query)\n",
    "        self.chat_history.add_ai_message(result[\"result\"])\n",
    "        return result[\"result\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
