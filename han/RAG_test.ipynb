{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36cc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def make_chroma_db(documents):\n",
    "    # Chunking\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    # ë²¡í„° ì €ì¥ì†Œ ë§Œë“¤ê¸°\n",
    "    db = Chroma.from_documents(docs, OpenAIEmbeddings(), persist_directory=\"chroma_db\")\n",
    "    return db\n",
    "\n",
    "def get_top5_docs_from_db(query):\n",
    "    db = Chroma(persist_directory=\"chroma_db\", embedding_function=OpenAIEmbeddings())\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 5}) # ìƒìœ„ 5ê°œë§Œ ì¶”ì¶œí•˜ë„ë¡ ì„¤ì •\n",
    "\n",
    "    return retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)\n",
    "client = OpenAI()\n",
    "\n",
    "# ğŸ” GPTë¥¼ ì‚¬ìš©í•´ ìš”ì•½ ìƒì„±\n",
    "def summarize_with_gpt(content: str, file_path: str, max_chars: int = 1500) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    ë‹¤ìŒì€ '{file_path}'ë¼ëŠ” íŒŒì¼ì˜ ì½”ë“œì…ë‹ˆë‹¤. \n",
    "    ì´ íŒŒì¼ì˜ ëª©ì ì´ ë¬´ì—‡ì¸ì§€, ì–´ë–¤ ê¸°ëŠ¥ì´ ìˆê³  ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ì§€ ê°„ë‹¨íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”. \n",
    "    \\n\\n```python\\n{content[:max_chars]}\\n```\\n\\nìš”ì•½:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",  # ë˜ëŠ” gpt-3.5-turbo\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPT ìš”ì•½ ì‹¤íŒ¨ ({file_path}): {e}\")\n",
    "        return \"ìš”ì•½ ì‹¤íŒ¨\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee5bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Git] Gitì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤: C:\\Program Files\\Git\\cmd\\git.EXE\n",
      "[ì˜¤ë¥˜] ì˜¬ë°”ë¥¸ GitHub ì €ì¥ì†Œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n",
      "ì˜ˆì‹œ: https://github.com/octocat/Hello-World\n",
      "ë˜ëŠ”: https://github.com/octocat/Hello-World/blob/main/README.md\n",
      "[ì˜¤ë¥˜] ì˜¬ë°”ë¥¸ GitHub ì €ì¥ì†Œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n",
      "ì˜ˆì‹œ: https://github.com/octocat/Hello-World\n",
      "ë˜ëŠ”: https://github.com/octocat/Hello-World/blob/main/README.md\n",
      "\n",
      "[ì •ë³´] ì €ì¥ì†Œ ì†Œìœ ì: AnsirH\n",
      "[ì •ë³´] ì €ì¥ì†Œ ì´ë¦„: LANGCHAIN\n",
      "\n",
      "[ì •ë³´] ì „ì²´ ì €ì¥ì†Œ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\USER\\Desktop\\GitHub\\3rd_project\")  # chahae í´ë”ì˜ ìƒìœ„ í´ë”\n",
    "\n",
    "from chahae.github_repo_viewer import main\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "documents = main(os.environ.get(\"GITHUB_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96354467",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = make_chroma_db(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b39d088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_path': '03_PromptTemplate.ipynb', 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'type': 'file', 'file_name': '03_PromptTemplate.ipynb', 'size': 16527}, page_content='\"# í…œí”Œë¦¿ ë¬¸ìì—´ ì •ì˜\\\\n\",\\n    \"template_str = (\\\\n\",\\n    \"    \\\\\"ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ ë§ˆì¼€íŒ… ì¹´í”¼ë¼ì´í„°ì…ë‹ˆë‹¤.\\\\\\\\n\\\\\"\\\\n\",\\n    \"    \\\\\"ì•„ë˜ ì œí’ˆì˜ ë§¤ë ¥ì ì¸ í™ë³´ ë¬¸êµ¬ë¥¼ 100ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\\\\\\\\n\\\\\\\\n\\\\\"\\\\n\",\\n    \"    \\\\\"ì œí’ˆ ëª…: {product_name}\\\\\\\\n\\\\\"\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"# í…œí”Œë¦¿ ê°ì²´ ìƒì„±\\\\n\",\\n    \"product_prompt = PromptTemplate.from_template(template_str)\\\\n\",\\n    \"\\\\n\",\\n    \"# í”„ë¡¬í”„íŠ¸ì— ì œí’ˆ ì´ë¦„ì„ ì‚½ì…\\\\n\",\\n    \"product_name = \\\\\"ìŠ¤ë§ˆíŠ¸í°\\\\\"\\\\n\",\\n    \"formatted_prompt = product_prompt.format(product_name=product_name)\\\\n\",\\n    \"# í”„ë¡¬í”„íŠ¸ ì¶œë ¥\\\\n\",\\n    \"print(formatted_prompt)\"'),\n",
       " Document(metadata={'file_name': '03_PromptTemplate.ipynb', 'type': 'file', 'size': 16527, 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'file_path': '03_PromptTemplate.ipynb'}, page_content='\"\\\\n\",\\n    \"# ì¶œë ¥ íŒŒì„œ ì„¤ì •\\\\n\",\\n    \"parser = StrOutputParser()\\\\n\",\\n    \"\\\\n\",\\n    \"# í…œí”Œë¦¿ì„ ì´ìš©í•´ì„œ ë¬¸ì¥ì„ ì™„ì„±\\\\n\",\\n    \"question = \\\\\"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"# í”„ë¡¬í”„íŠ¸ | llm | parser\\\\n\",\\n    \"chain = chat_prompt | llm | parser\\\\n\",\\n    \"\\\\n\",\\n    \"response = chain.invoke({\\\\\"question\\\\\": question})\\\\n\",\\n    \"# ì‘ë‹µ ì¶œë ¥\\\\n\",\\n    \"print(response)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"id\": \"17378408\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# PartialPromptTemplate\\\\n\",'),\n",
       " Document(metadata={'type': 'file', 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'file_path': '03_PromptTemplate.ipynb', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'file_name': '03_PromptTemplate.ipynb', 'size': 16527}, page_content='\"source\": [\\n    \"# PartialPromptTemplate\\\\n\",\\n    \"- í…œí”Œë¦¿ì˜ ì¼ë¶€ë¥¼ ë¶€ë¶„ì ìœ¼ë¡œ ì±„ìš´ ìƒˆë¡œìš´ í…œí”Œë¦¿\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 20,\\n   \"id\": \"0fab1819\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\\\\n\",\\n    \"role_system_template = \\\\\"ë‹¹ì‹ ì€ {rule} ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ì¸ ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ìì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.\\\\\"\\\\n\",\\n    \"system_prompt =SystemMessagePromptTemplate.from_template(role_system_template)\\\\n\",'),\n",
       " Document(metadata={'file_name': '01_LCEL.ipynb', 'size': 8395, 'type': 'file', 'file_path': '01_LCEL.ipynb', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/01_LCEL.ipynb', 'sha': '81012205169ca77d0d5e37269a0b158156498b2c'}, page_content='\"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"ì»¤í”¼ ì œí’ˆì„ ìƒì‚°í•˜ëŠ” íšŒì‚¬ ì´ë¦„ì€ ë­˜ë¡œ í•˜ë©´ ì¢‹ì„ê¹Œ?\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from langchain_core.prompts import PromptTemplate\\\\n\",\\n    \"\\\\n\",\\n    \"template = \\\\\"{product} ì œí’ˆì„ ìƒì‚°í•˜ëŠ” íšŒì‚¬ ì´ë¦„ì€ ë­˜ë¡œ í•˜ë©´ ì¢‹ì„ê¹Œ?\\\\\"\\\\n\",\\n    \"prompt = PromptTemplate.from_template(template)\\\\n\",\\n    \"formated_prompt = prompt.format(product=\\\\\"ì»¤í”¼\\\\\")\\\\n\",\\n    \"print(formated_prompt)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",'),\n",
       " Document(metadata={'file_path': '03_PromptTemplate.ipynb', 'size': 16527, 'file_name': '03_PromptTemplate.ipynb', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'type': 'file'}, page_content='\"# summary_prompt = PromptTemplate.from_template(multi_template_str)\\\\n\",\\n    \"\\\\n\",\\n    \"# í¬ë©§íŒ…ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ ê°’ ì„¤ì •\\\\n\",\\n    \"title = \\\\\"AI ê¸°ìˆ ì˜ ë°œì „ê³¼ ë¯¸ë˜\\\\\"\\\\n\",\\n    \"keywords = \\\\\"ì¸ê³µì§€ëŠ¥, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹\\\\\"\\\\n\",\\n    \"formatted_summary_prompt = summary_prompt.format(title=title, keywords=keywords)\\\\n\",\\n    \"\\\\n\",\\n    \"# í”„ë¡¬í”„íŠ¸ ì¶œë ¥\\\\n\",\\n    \"print(formatted_summary_prompt)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 12,\\n   \"id\": \"76c98f79\",\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"prompt template ê´€ë ¨ ì½”ë“œ\"\n",
    "results = get_top5_docs_from_db(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403b4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24300\\1783291465.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24300\\1783291465.py:9: LangChainDeprecationWarning: This function is deprecated. Refer to this guide on retrieval and question answering with sources: https://python.langchain.com/docs/how_to/qa_sources/\n",
      "See also the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "  qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24300\\1783291465.py:10: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì„¤ëª… ê²°ê³¼:\n",
      " The code related to the prompt template includes defining a template string, creating a template object, inserting a product name into the prompt, and printing the formatted prompt. It also includes setting up an output parser, using the template to complete a sentence, and printing the response. There are also examples of creating a PartialPromptTemplate, which is a new template that partially fills in part of the template. Other examples include formatting the prompt value through formatting and printing the prompt.\n",
      "SOURCES: https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb, https://github.com/AnsirH/LANGCHAIN/blob/main/01_LCEL.ipynb\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "\n",
    "# 4. LLM ì¤€ë¹„\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# 5. ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª… ìƒì„± (Chain ì‚¬ìš©)\n",
    "qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "# 6. ì¶œë ¥\n",
    "print(\"ğŸ“ ì„¤ëª… ê²°ê³¼:\\n\", result[\"output_text\"])\n",
    "# print(\"\\nğŸ“š ì°¸ê³ ëœ ë¬¸ì„œ ì •ë³´:\\n\", result[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38b91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': 'í•˜ë…¸ì´íƒ‘ ì½”ë“œ ê²½ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: [ë°”ë¡œê°€ê¸°](https://github.com/wonwookim/coding_test_study/tree/main/week_2)\\nSOURCES: https://github.com/hwangchahae/coding_test_study/blob/main/README.md'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc581aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
