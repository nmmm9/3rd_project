{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36cc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def make_chroma_db(documents):\n",
    "    # Chunking\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    # Î≤°ÌÑ∞ Ï†ÄÏû•ÏÜå ÎßåÎì§Í∏∞\n",
    "    db = Chroma.from_documents(docs, OpenAIEmbeddings(), persist_directory=\"chroma_db\")\n",
    "    return db\n",
    "\n",
    "def get_top5_docs_from_db(query):\n",
    "    db = Chroma(persist_directory=\"chroma_db\", embedding_function=OpenAIEmbeddings())\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 5}) # ÏÉÅÏúÑ 5Í∞úÎßå Ï∂îÏ∂úÌïòÎèÑÎ°ù ÏÑ§Ï†ï\n",
    "\n",
    "    return retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API ÌÇ§ ÏÑ§Ï†ï (ÌôòÍ≤ΩÎ≥ÄÏàò ÎòêÎäî ÏßÅÏ†ë ÏûÖÎ†•)\n",
    "client = OpenAI()\n",
    "\n",
    "# üîç GPTÎ•º ÏÇ¨Ïö©Ìï¥ ÏöîÏïΩ ÏÉùÏÑ±\n",
    "def summarize_with_gpt(content: str, file_path: str, max_chars: int = 1500) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Îã§ÏùåÏùÄ '{file_path}'ÎùºÎäî ÌååÏùºÏùò ÏΩîÎìúÏûÖÎãàÎã§. \n",
    "    Ïù¥ ÌååÏùºÏùò Î™©Ï†ÅÏù¥ Î¨¥ÏóáÏù∏ÏßÄ, Ïñ¥Îñ§ Í∏∞Îä•Ïù¥ ÏûàÍ≥† Ïñ¥Îñ§ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÎäîÏßÄ Í∞ÑÎã®Ìûà ÏöîÏïΩÌï¥ Ï£ºÏÑ∏Ïöî. \n",
    "    \\n\\n```python\\n{content[:max_chars]}\\n```\\n\\nÏöîÏïΩ:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",  # ÎòêÎäî gpt-3.5-turbo\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GPT ÏöîÏïΩ Ïã§Ìå® ({file_path}): {e}\")\n",
    "        return \"ÏöîÏïΩ Ïã§Ìå®\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eee5bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ÏÑ§Ïπò] python-dotenv Ìå®ÌÇ§ÏßÄÎ•º ÏÑ§ÏπòÌï©ÎãàÎã§...\n",
      "[ÏÑ§Ïπò] python-dotenv Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÏôÑÎ£å\n",
      "[Git] GitÏù¥ ÏÑ§ÏπòÎêòÏñ¥ ÏûàÏäµÎãàÎã§: C:\\Program Files\\Git\\cmd\\git.EXE\n",
      "\n",
      "[Ï†ïÎ≥¥] Ï†ÄÏû•ÏÜå ÏÜåÏú†Ïûê: hwangchahae\n",
      "[Ï†ïÎ≥¥] Ï†ÄÏû•ÏÜå Ïù¥Î¶Ñ: coding_test_study\n",
      "\n",
      "[Ï†ïÎ≥¥] Ï†ÑÏ≤¥ Ï†ÄÏû•ÏÜå ÎÇ¥Ïö©ÏùÑ Í∞ÄÏ†∏Ïò§Îäî Ï§ë...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\USER\\Desktop\\GitHub\\3rd_project\")  # chahae Ìè¥ÎçîÏùò ÏÉÅÏúÑ Ìè¥Îçî\n",
    "\n",
    "from chahae.github_repo_viewer import main\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "documents = main(os.environ.get(\"GITHUB_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96354467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\2972093822.py:12: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  db = Chroma.from_documents(docs, OpenAIEmbeddings(), persist_directory=\"chroma_db\")\n"
     ]
    }
   ],
   "source": [
    "chroma_db = make_chroma_db(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b39d088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\2972093822.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=\"chroma_db\", embedding_function=OpenAIEmbeddings())\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\2972093822.py:19: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_name': 'README.md', 'source': 'week_10/1697_Ïà®Î∞îÍº≠Ïßà/README.md'}, page_content='# Ïà®Î∞îÍº≠Ïßà Î¨∏Ï†ú ÌíÄÏù¥\\n\\nÎ∞±Ï§Ä Î¨∏Ï†ú Î≤àÌò∏: 1697'),\n",
       " Document(metadata={'source': 'week_8/15683_Í∞êÏãú/README.md', 'file_name': 'README.md'}, page_content='# Í∞êÏãú Î¨∏Ï†ú ÌíÄÏù¥\\n\\nÎ∞±Ï§Ä Î¨∏Ï†ú Î≤àÌò∏: 15683'),\n",
       " Document(metadata={'source': 'README.md', 'file_name': 'README.md'}, page_content='# ÏïåÍ≥†Î¶¨Ï¶ò ÌíÄÏù¥ (2025.03.27 ~)\\n\\n## üìå Ïù¥Î≤à Ï£º Î¨∏Ï†ú'),\n",
       " Document(metadata={'source': 'README.md', 'file_name': 'README.md'}, page_content='## üìå Ïä§ÌÑ∞Îîî Î∞©Ïãù\\n1. ÌíÄ ÌîåÎû´ÌèºÏùÄ [Î∞±Ï§Ä](https://www.acmicpc.net/)ÏùÑ ÏÇ¨Ïö©ÌïúÎã§.\\n2. ÏïåÍ≥†Î¶¨Ï¶ò Ïä§ÌÑ∞ÎîîÎäî Ï†ïÌï¥ÏßÑ Î¨∏Ï†úÎ•º ÌëºÎã§.\\n3. Í∞ÅÏûê Î¨∏Ï†ú ÌíÄÏù¥ ÌõÑ Ïä§ÌÑ∞Îîî Î™®ÏûÑ Îïå ÏΩîÎìú Î¶¨Î∑∞Î•º ÏßÑÌñâÌïúÎã§.\\n4. Ïä§ÌÑ∞Îîî ÏßÑÌñâ Î∞©Ïãù Ï∞∏Í≥†: [ÏΩîÎî©ÌÖåÏä§Ìä∏ ÏßÑÌñâ Î∞©Ïãù](https://dev-dain.tistory.com/155)\\n\\n## üèÜ Î¨∏Ï†ú ÌíÄÏù¥ Í∏∞Î°ù'),\n",
       " Document(metadata={'file_name': 'README.md', 'source': 'week_10/1012_Ïú†Í∏∞ÎÜç_Î∞∞Ï∂î/README.md'}, page_content='# Ïú†Í∏∞ÎÜç Î∞∞Ï∂î Î¨∏Ï†ú ÌíÄÏù¥\\n\\nÎ∞±Ï§Ä Î¨∏Ï†ú Î≤àÌò∏: 1012')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"ÌÅê Í¥ÄÎ†® Î¨∏Ï†ú ÏΩîÎìú Ï∞æÏïÑÏ§ò\"\n",
    "results = get_top5_docs_from_db(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0890605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'README.md', 'source': 'week_10/1697_Ïà®Î∞îÍº≠Ïßà/README.md'}\n"
     ]
    }
   ],
   "source": [
    "print(results[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403b4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\1783291465.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\1783291465.py:9: LangChainDeprecationWarning: This function is deprecated. Refer to this guide on retrieval and question answering with sources: https://python.langchain.com/docs/how_to/qa_sources/\n",
      "See also the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "  qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_10732\\1783291465.py:10: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù ÏÑ§Î™Ö Í≤∞Í≥º:\n",
      " I'm sorry, but the provided contents do not contain any information about the code for a problem related to \"ÌÅê\".\n",
      "SOURCES:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "\n",
    "# 4. LLM Ï§ÄÎπÑ\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# 5. Î¨∏ÏÑúÎ•º Í∏∞Î∞òÏúºÎ°ú ÏÑ§Î™Ö ÏÉùÏÑ± (Chain ÏÇ¨Ïö©)\n",
    "qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "# 6. Ï∂úÎ†•\n",
    "print(\"üìù ÏÑ§Î™Ö Í≤∞Í≥º:\\n\", result[\"output_text\"])\n",
    "# print(\"\\nüìö Ï∞∏Í≥†Îêú Î¨∏ÏÑú Ï†ïÎ≥¥:\\n\", result[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac38b91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided contents do not contain any information about the code for a problem related to \"ÌÅê\".\n",
      "SOURCES:\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc581aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class MemoryRAGRetriever:\n",
    "    def __init__(self, collection: Chroma):\n",
    "        self.vectorstore = collection\n",
    "        self.retriever = self.vectorstore.as_retriever()\n",
    "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "        self.retriever_with_memory = create_history_aware_retriever(\n",
    "            llm=self.llm,\n",
    "            retriever=self.retriever,\n",
    "            prompt=PromptTemplate.from_template(\"\"\"\n",
    "Îã§ÏùåÏùÄ ÏÇ¨Ïö©ÏûêÏôÄÏùò ÏµúÍ∑º ÎåÄÌôî ÏùºÎ∂ÄÏûÖÎãàÎã§:\n",
    "{chat_history}\n",
    "\n",
    "ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏: {input}\n",
    "\n",
    "ÏúÑ ÏßàÎ¨∏Ïóê Í∞ÄÏû• Í¥ÄÎ†®Îêú ÎÇ¥Ïö©ÏùÑ Ï∞æÍ∏∞ ÏúÑÌïú ÏøºÎ¶¨Î•º ÎßåÎì§Ïñ¥Ï£ºÏÑ∏Ïöî.\n",
    "\"\"\")\n",
    "        )\n",
    "\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            retriever=self.retriever_with_memory,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        self.chat_history = ChatMessageHistory()\n",
    "\n",
    "    def query(self, user_query: str) -> str:\n",
    "        # ÏµúÍ∑º 3ÏåçÎßå Ïú†ÏßÄ (user + ai = 6Í∞ú)\n",
    "        recent_history = self.chat_history.messages[-6:]\n",
    "        \n",
    "        result = self.qa_chain.invoke({\n",
    "            \"input\": user_query,\n",
    "            \"chat_history\": recent_history\n",
    "        })\n",
    "\n",
    "        self.chat_history.add_user_message(user_query)\n",
    "        self.chat_history.add_ai_message(result[\"result\"])\n",
    "        return result[\"result\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
