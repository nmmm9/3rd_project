"""
GitHub ì €ì¥ì†Œ ë¶„ì„ ë° ì„ë² ë”©ì„ ìœ„í•œ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ GitHub ì €ì¥ì†Œì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ë¶„ì„í•˜ê³ , 
LangChain Documentë¡œ ë³€í™˜í•œ í›„ ChromaDBì— ì„ë² ë”©í•˜ì—¬ ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” í´ë˜ìŠ¤:
    - GitHubRepositoryFetcher: GitHub ì €ì¥ì†Œì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ëŠ” í´ë˜ìŠ¤
    - RepositoryEmbedder: ì €ì¥ì†Œ ë‚´ìš©ì„ ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤

ì£¼ìš” í•¨ìˆ˜:
    - analyze_repository: GitHub ì €ì¥ì†Œë¥¼ ë¶„ì„í•˜ê³  ì„ë² ë”©í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
"""

import requests
import chromadb
import os
import re
import openai
import git
import base64
from typing import Optional, List, Dict, Any, Tuple
from langchain.schema import Document
from cryptography.fernet import Fernet
import tiktoken
import ast
import markdown
import concurrent.futures
import asyncio
import sys

# ----------------- ìƒìˆ˜ ì •ì˜ -----------------
MAIN_EXTENSIONS = ['.py', '.js', '.md']  # ë¶„ì„í•  ì£¼ìš” íŒŒì¼ í™•ì¥ì
CHUNK_SIZE = 500  # í…ìŠ¤íŠ¸ ì²­í¬ í¬ê¸°
GITHUB_TOKEN = "GITHUB_TOKEN"  # í™˜ê²½ ë³€ìˆ˜ í‚¤ ì´ë¦„
KEY_FILE = ".key"  # ì•”í˜¸í™” í‚¤ íŒŒì¼

# ChromaDB ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬)
chroma_client = chromadb.Client()

def analyze_repository(repo_url: str, token: Optional[str] = None, session_id: Optional[str] = None) -> Dict[str, Any]:
    """
    GitHub ì €ì¥ì†Œë¥¼ ë¶„ì„í•˜ê³  ì„ë² ë”©í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ë™ì‘í•©ë‹ˆë‹¤:
    1. GitHub ì €ì¥ì†Œë¥¼ ë¡œì»¬ì— í´ë¡ 
    2. ì£¼ìš” íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ í•„í„°ë§ (MAIN_EXTENSIONSì— ì •ì˜ëœ í™•ì¥ìë§Œ)
    3. íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ì„ë² ë”© ì²˜ë¦¬
    4. ë””ë ‰í† ë¦¬ êµ¬ì¡° íŠ¸ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        repo_url (str): ë¶„ì„í•  GitHub ì €ì¥ì†Œ URL
        token (Optional[str]): GitHub ê°œì¸ ì•¡ì„¸ìŠ¤ í† í°
        session_id (Optional[str]): ì„¸ì…˜ ID (ê¸°ë³¸ê°’: owner_repo)
        
    Returns:
        Dict[str, Any]:
            'files': ë¶„ì„ëœ íŒŒì¼ ëª©ë¡ (ê° íŒŒì¼ì€ {'path': '...', 'content': '...'} í˜•ì‹)
            'directory_structure': ë””ë ‰í† ë¦¬ êµ¬ì¡° íŠ¸ë¦¬ í…ìŠ¤íŠ¸
        
    Raises:
        ValueError: ì˜ëª»ëœ GitHub URLì¸ ê²½ìš°
        Exception: ì €ì¥ì†Œ í´ë¡  ì‹¤íŒ¨ ì‹œ
    """
    try:
        # 1. Git ì €ì¥ì†Œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        fetcher = GitHubRepositoryFetcher(repo_url, token, session_id)
        fetcher.clone_repo()
        
        # 2. ì£¼ìš” íŒŒì¼ í•„í„°ë§ ë° ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        fetcher.filter_main_files()  # MAIN_EXTENSIONSì— ì •ì˜ëœ í™•ì¥ìë§Œ í•„í„°ë§
        files = fetcher.get_file_contents()

        # 3. ë°ì´í„° ì„ë² ë”© ì²˜ë¦¬
        embedder = RepositoryEmbedder(fetcher.session_id)
        embedder.process_and_embed(files)

        # 4. ë””ë ‰í† ë¦¬ êµ¬ì¡° íŠ¸ë¦¬ í…ìŠ¤íŠ¸ ìƒì„±
        directory_structure = fetcher.generate_directory_structure()
        
        return {
            'files': files,
            'directory_structure': directory_structure
        }
        
    except ValueError as e:
        print(f"[ì˜¤ë¥˜] ì˜ëª»ëœ GitHub URL: {e}")
        raise
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ì €ì¥ì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        raise

class GitHubRepositoryFetcher:
    """
    GitHub ì €ì¥ì†Œì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¤ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” GitHub APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ì†Œì˜ íŒŒì¼ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê³ ,
    LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self, repo_url: str, token: Optional[str] = None, session_id: Optional[str] = None):
        """
        GitHub ì €ì¥ì†Œ ë·°ì–´ ì´ˆê¸°í™”
        
        Args:
            repo_url (str): GitHub ì €ì¥ì†Œ URL
            token (Optional[str]): GitHub ê°œì¸ ì•¡ì„¸ìŠ¤ í† í°
            session_id (Optional[str]): ì„¸ì…˜ ID (ê¸°ë³¸ê°’: owner_repo)
        """
        self.repo_url = repo_url
        self.token = token
        self.headers = {'Authorization': f'token {token}'} if token else {}
        self.files = []
        
        # ì €ì¥ì†Œ ì •ë³´ ì¶”ì¶œ
        self.owner, self.repo, self.path = self.extract_repo_info(repo_url)
        if not self.owner or not self.repo:
            raise ValueError("Invalid GitHub repository URL")
            
        # ì„¸ì…˜ ë° ì €ì¥ì†Œ ê²½ë¡œ ì„¤ì •
        self.session_id = session_id or f"{self.owner}_{self.repo}"
        self.repo_path = f"./repos/{self.session_id}"
        
        # ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self.collection = chroma_client.get_or_create_collection(
            name=self.session_id,
            metadata={"description": f"Repository: {self.owner}/{self.repo}"}
        )

    def create_error_response(self, message: str, status_code: int) -> Dict[str, Any]:
        """
        API ì—ëŸ¬ ì‘ë‹µ ìƒì„±
        
        Args:
            message (str): ì—ëŸ¬ ë©”ì‹œì§€
            status_code (int): HTTP ìƒíƒœ ì½”ë“œ
            
        Returns:
            Dict[str, Any]: ì—ëŸ¬ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        """
        return {
            'error': True,
            'message': message,
            'status_code': status_code
        }

    def handle_github_response(self, response: requests.Response, path: str = None) -> Dict[str, Any]:
        """
        GitHub API ì‘ë‹µ ì²˜ë¦¬
        
        Args:
            response (requests.Response): GitHub API ì‘ë‹µ
            path (str, optional): ìš”ì²­í•œ íŒŒì¼/ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            Dict[str, Any]: ì²˜ë¦¬ëœ ì‘ë‹µ ë°ì´í„° ë˜ëŠ” ì—ëŸ¬ ì •ë³´
        """
        if response.status_code == 403:
            return self.create_error_response(
                'GitHub API í˜¸ì¶œ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                403
            )
            
        if response.status_code == 404:
            return self.create_error_response(
                f'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}' if path else 'ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                404
            )
            
        if response.status_code == 401:
            return self.create_error_response(
                'ë¹„ê³µê°œ ì €ì¥ì†Œì— ì ‘ê·¼í•˜ë ¤ë©´ GitHub í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.',
                401
            )
            
        if response.status_code != 200:
            return self.create_error_response(
                f'GitHub API ì˜¤ë¥˜: {response.text}',
                response.status_code
            )
        
        return response.json()

    def extract_repo_info(self, url: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        GitHub URLì—ì„œ ì†Œìœ ì, ì €ì¥ì†Œ ì´ë¦„, íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ì¶œ
        
        Args:
            url (str): GitHub ì €ì¥ì†Œ URL
            
        Returns:
            Tuple[Optional[str], Optional[str], Optional[str]]: 
                (owner, repo, path) ë˜ëŠ” (None, None, None)
        """
        try:
            # URL ì •ê·œí™”
            url = url.strip().rstrip('/')
            if url.endswith('.git'):
                url = url[:-4]
                
            # URL íŒŒì‹±
            parts = url.split('/')
            if 'github.com' in parts:
                github_index = parts.index('github.com')
                if len(parts) >= github_index + 3:
                    owner = parts[github_index + 1]
                    repo = parts[github_index + 2]
                    path = '/'.join(parts[github_index + 3:]) if len(parts) > github_index + 3 else None
                    return owner, repo, path
        except Exception as e:
            print(f"URL íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None

    def clone_repo(self):
        """
        GitHub ì €ì¥ì†Œë¥¼ ë¡œì»¬ì— í´ë¡ 
        
        Raises:
            Exception: í´ë¡  ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        """
        if not os.path.exists(self.repo_path):
            try:
                git.Repo.clone_from(self.repo_url, self.repo_path)
            except Exception as e:
                print("[DEBUG] GitHub í´ë¡  ì—ëŸ¬:", e)
                raise

    def get_repo_directory_contents(self, path: str = "") -> Optional[List[Dict[str, Any]]]:
        """
        GitHub APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ì†Œì˜ ë””ë ‰í† ë¦¬ ë‚´ìš©ì„ ê°€ì ¸ì˜´
        
        Args:
            path (str): ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: ë£¨íŠ¸ ë””ë ‰í† ë¦¬)
            
        Returns:
            Optional[List[Dict[str, Any]]]: 
                ë””ë ‰í† ë¦¬ ë‚´ìš© ëª©ë¡ ë˜ëŠ” ì—ëŸ¬ ì •ë³´
                ê° í•­ëª©ì€ GitHub API ì‘ë‹µ í˜•ì‹ì˜ íŒŒì¼/ë””ë ‰í† ë¦¬ ì •ë³´
        """
        try:
            # API í˜¸ì¶œ ì¤€ë¹„
            url = f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{path}"
            headers = {
                "Accept": "application/vnd.github.v3+json"
            }
            if self.token:
                headers["Authorization"] = f"token {self.token}"
            
            # API ìš”ì²­ ì‹¤í–‰
            response = requests.get(url, headers=headers)
            content = self.handle_github_response(response, path)
            
            # ì‘ë‹µ ê²€ì¦
            if isinstance(content, dict) and content.get('error'):
                return content
            if isinstance(content, list):
                return content
            return self.create_error_response("ì˜ëª»ëœ ì‘ë‹µ í˜•ì‹", 500)
            
        except requests.exceptions.RequestException as e:
            return self.create_error_response(f'API ìš”ì²­ ì‹¤íŒ¨: {str(e)}', 500)
        except Exception as e:
            return self.create_error_response(f'ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}', 500)
            
    def get_repo_content_as_document(self, path: str) -> Optional[Document]:
        """
        GitHub APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ì†Œì˜ íŒŒì¼ ë‚´ìš©ì„ LangChain Documentë¡œ ê°€ì ¸ì˜´
        
        Args:
            path (str): íŒŒì¼ ê²½ë¡œ
        
        Returns:
            Optional[Document]: 
                LangChain Document ê°ì²´ ë˜ëŠ” None (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
                DocumentëŠ” íŒŒì¼ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨
        """
        try:
            # API í˜¸ì¶œ ì¤€ë¹„
            url = f"https://api.github.com/repos/{self.owner}/{self.repo}/contents/{path}"
            headers = {
                "Accept": "application/vnd.github.v3+json"
            }
            if self.token:
                headers["Authorization"] = f"token {self.token}"
            
            # API ìš”ì²­ ì‹¤í–‰
            response = requests.get(url, headers=headers)
            content_data = self.handle_github_response(response, path)
            
            # ì—ëŸ¬ ì²´í¬
            if not content_data or isinstance(content_data, dict) and content_data.get('error'):
                return None
            
            # Base64 ë””ì½”ë”©
            content = base64.b64decode(content_data['content']).decode('utf-8')
            
            # Document ê°ì²´ ìƒì„±
            return Document(
                page_content=content,
                metadata={
                    'source': content_data['html_url'],
                    'file_name': content_data['name'],
                    'file_path': content_data['path'],
                    'sha': content_data['sha'],
                    'size': content_data['size'],
                    'type': content_data['type']
                }
            )
        except Exception as e:
            print(f"Document ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def get_repo_directory_as_documents(self, path: str = "") -> List[Document]:
        """
        GitHub APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ì†Œì˜ ë””ë ‰í† ë¦¬ ë‚´ìš©ì„ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜´
        
        Args:
            path (str): ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: ë£¨íŠ¸ ë””ë ‰í† ë¦¬)
            
        Returns:
            List[Document]: 
                LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
                ê° DocumentëŠ” íŒŒì¼ì˜ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨
        """
        documents = []
        try:
            # ë””ë ‰í† ë¦¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            dir_contents = self.get_repo_directory_contents(path)
            if not dir_contents:
                return documents
                
            # ê° í•­ëª© ì²˜ë¦¬
            for item in dir_contents:
                if item['type'] == 'file':
                    # íŒŒì¼ì¸ ê²½ìš° Documentë¡œ ë³€í™˜
                    doc = self.get_repo_content_as_document(item['path'])
                    if doc:
                        documents.append(doc)
                elif item['type'] == 'dir':
                    # ë””ë ‰í† ë¦¬ì¸ ê²½ìš° ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                    sub_docs = self.get_repo_directory_as_documents(item['path'])
                    documents.extend(sub_docs)
                    
            return documents
        except Exception as e:
            print(f"[API] Document ë¦¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return documents

    def get_all_repo_contents(self) -> List[Document]:
        """
        GitHub ì €ì¥ì†Œì˜ ëª¨ë“  íŒŒì¼ê³¼ í´ë”ë¥¼ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜´
        
        Returns:
            List[Document]: ëª¨ë“  íŒŒì¼ì˜ LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        return self.get_repo_directory_as_documents()

    def get_all_main_files(self, path=""):
        files = []
        dir_contents = self.get_repo_directory_contents(path)
        if isinstance(dir_contents, list):
            for item in dir_contents:
                if item['type'] == 'file' and any(item['path'].endswith(ext) for ext in MAIN_EXTENSIONS):
                    files.append(item['path'])
                elif item['type'] == 'dir':
                    files.extend(self.get_all_main_files(item['path']))
        return files

    def filter_main_files(self):
        self.files = self.get_all_main_files()
        print(f"[DEBUG] í•„í„°ë§ëœ ì£¼ìš” íŒŒì¼: {self.files}")
        print(f"[DEBUG] ì£¼ìš” íŒŒì¼ ê°œìˆ˜: {len(self.files)}")

    def get_file_contents(self) -> List[Dict[str, Any]]:
        """
        ì£¼ìš” íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        Returns:
            List[Dict[str, Any]]: 
                íŒŒì¼ ê²½ë¡œì™€ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
                [{'path': '...', 'content': '...', 'file_name': ..., 'file_type': ..., 'sha': ..., 'source_url': ...}, ...]
        """
        file_objs = []
        for path in self.files:
            doc = self.get_repo_content_as_document(path)
            if doc:
                meta = doc.metadata
                file_objs.append({
                    'path': path,
                    'content': doc.page_content,
                    'file_name': meta.get('file_name'),
                    'file_type': meta.get('file_name', '').split('.')[-1] if meta.get('file_name') else '',
                    'sha': meta.get('sha'),
                    'source_url': meta.get('source'),
                })
        return file_objs

    def generate_directory_structure(self) -> str:
        """
        ì €ì¥ì†Œì˜ ì „ì²´ ë””ë ‰í† ë¦¬/íŒŒì¼ êµ¬ì¡°ë¥¼ íŠ¸ë¦¬ í˜•íƒœì˜ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        """
        # ë””ë ‰í† ë¦¬ ë‚´ìš© ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        def build_tree(path=""):
            items = self.get_repo_directory_contents(path)
            tree = {}
            if not items or isinstance(items, dict) and items.get('error'):
                return tree
            for item in items:
                if item['type'] == 'file':
                    tree[f"ğŸ“„ {item['name']}"] = None
                elif item['type'] == 'dir':
                    tree[f"ğŸ“ {item['name']}"] = build_tree(item['path'])
            return tree
        
        tree = build_tree()
        lines = []
        def traverse(node, prefix=""):
            for key, value in sorted(node.items()):
                lines.append(f"{prefix}{key}")
                if value is not None:
                    traverse(value, prefix + "  ")
        traverse(tree)
        return "\n".join(lines)

    # ----------------- í† í° ê´€ë ¨ ê¸°ëŠ¥ -----------------
    @staticmethod
    def generate_key() -> bytes:
        """
        ì•”í˜¸í™” í‚¤ ìƒì„±
        
        Returns:
            bytes: ìƒì„±ëœ ì•”í˜¸í™” í‚¤
        """
        if not os.path.exists(KEY_FILE):
            key = Fernet.generate_key()
            with open(KEY_FILE, 'wb') as key_file:
                key_file.write(key)
            return key
        else:
            with open(KEY_FILE, 'rb') as key_file:
                return key_file.read()

    @staticmethod
    def encrypt_token(token: str) -> str:
        """
        í† í° ì•”í˜¸í™”
        
        Args:
            token (str): ì•”í˜¸í™”í•  í† í°
            
        Returns:
            str: ì•”í˜¸í™”ëœ í† í°
        """
        key = GitHubRepositoryFetcher.generate_key()
        f = Fernet(key)
        return f.encrypt(token.encode()).decode()

    @staticmethod
    def decrypt_token(encrypted_token: str) -> str:
        """
        í† í° ë³µí˜¸í™”
        
        Args:
            encrypted_token (str): ë³µí˜¸í™”í•  í† í°
            
        Returns:
            str: ë³µí˜¸í™”ëœ í† í°
        """
        key = GitHubRepositoryFetcher.generate_key()
        f = Fernet(key)
        return f.decrypt(encrypted_token.encode()).decode()

    @staticmethod
    def update_token(token: str) -> bool:
        """
        í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ì— GitHub í† í° ì—…ë°ì´íŠ¸
        
        Args:
            token (str): ì—…ë°ì´íŠ¸í•  í† í°
            
        Returns:
            bool: ì—…ë°ì´íŠ¸ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í† í° ì•”í˜¸í™”
            encrypted_token = GitHubRepositoryFetcher.encrypt_token(token)
            
            # ê¸°ì¡´ ë‚´ìš© ì½ê¸°
            with open(".env", 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # GitHub í† í° ì°¾ì•„ì„œ êµì²´
            token_found = False
            for i, line in enumerate(lines):
                if line.startswith(f"{GITHUB_TOKEN}="):
                    lines[i] = f"{GITHUB_TOKEN}={encrypted_token}\n"
                    token_found = True
                    break
            
            # í† í°ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€
            if not token_found:
                lines.append(f"{GITHUB_TOKEN}={encrypted_token}\n")
            
            # íŒŒì¼ ë‹¤ì‹œ ì“°ê¸°
            with open(".env", 'w', encoding='utf-8') as f:
                f.writelines(lines)
            
            return True
        except Exception as e:
            print(f"[ì˜¤ë¥˜] í† í° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False


class RepositoryEmbedder:
    """
    ì €ì¥ì†Œ ë‚´ìš©ì„ ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” GitHub ì €ì¥ì†Œì˜ íŒŒì¼ ë‚´ìš©ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê³ ,
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©í•œ í›„ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, session_id: str):
        """
        ì„ë² ë” ì´ˆê¸°í™”
        
        Args:
            session_id (str): ì„¸ì…˜ ID
        """
        self.session_id = session_id
        self.collection = chroma_client.get_or_create_collection(name=f"repo_{session_id}")

    def process_and_embed(self, files: List[Dict[str, Any]]):
        # ë‚´ë¶€ ë¹„ë™ê¸° í•¨ìˆ˜ ì •ì˜
        async def async_process_and_embed(files):
            import openai
            api_key = os.environ.get("OPENAI_API_KEY")
            client = openai.AsyncClient(api_key=api_key)
            enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
            def safe_meta(meta):
                return {k: ('' if v is None else v if not isinstance(v, (int, float, bool)) else v) for k, v in meta.items()}
            def split_by_tokens(text, max_tokens=256, overlap=64):
                tokens = enc.encode(text)
                chunks = []
                start = 0
                while start < len(tokens):
                    end = min(start + max_tokens, len(tokens))
                    chunk = enc.decode(tokens[start:end])
                    chunks.append((chunk, start, end))
                    if end == len(tokens):
                        break
                    start += max_tokens - overlap
                return chunks
            def chunk_python_functions(source_code):
                try:
                    tree = ast.parse(source_code)
                except Exception as e:
                    print(f"[WARNING] AST íŒŒì‹± ì‹¤íŒ¨: {e}")
                    return [(source_code, 0, len(enc.encode(source_code)), None, None, 1, len(source_code.splitlines()), None, 0, None)]
                
                lines = source_code.splitlines()
                chunks = []
                imports = []
                parent_map = {}  # ë¶€ëª¨-ìì‹ ê´€ê³„ ì¶”ì 
                
                # ë¶€ëª¨-ìì‹ ê´€ê³„ ë§µ êµ¬ì¶•
                for node in ast.walk(tree):
                    for child in ast.iter_child_nodes(node):
                        parent_map[child] = node
                
                # ì„í¬íŠ¸ ë¬¸ ìˆ˜ì§‘
                for node in tree.body:
                    if isinstance(node, (ast.Import, ast.ImportFrom)):
                        start = node.lineno - 1
                        end = getattr(node, 'end_lineno', start + 1)
                        import_text = '\n'.join(lines[start:end])
                        imports.append(import_text)
                
                # ì „ì²´ ì„í¬íŠ¸ ë¬¸ìì—´
                imports_text = '\n'.join(imports)
                
                # ë³µì¡ë„ ê³„ì‚° í•¨ìˆ˜
                def calculate_complexity(node):
                    """AST ë…¸ë“œì˜ ë³µì¡ë„ ê³„ì‚°"""
                    if isinstance(node, ast.FunctionDef):
                        # ê¸°ë³¸ ë³µì¡ë„ + íŒŒë¼ë¯¸í„° ìˆ˜ + ë‚´ë¶€ ì¡°ê±´ë¬¸/ë°˜ë³µë¬¸ ìˆ˜
                        complexity = 1 + len(node.args.args)
                        for n in ast.walk(node):
                            if isinstance(n, (ast.If, ast.For, ast.While, ast.Try)):
                                complexity += 1
                        return complexity
                    elif isinstance(node, ast.ClassDef):
                        # ê¸°ë³¸ ë³µì¡ë„ + ìƒì† ìˆ˜ + ë©”ì†Œë“œ ìˆ˜
                        complexity = 1 + len(node.bases)
                        for n in node.body:
                            if isinstance(n, ast.FunctionDef):
                                complexity += 1
                        return complexity
                    return 1
                
                # ê³„ì¸µì  ì²­í‚¹ í•¨ìˆ˜
                def process_node(node, parent_class=None, parent_func=None, depth=0):
                    """ë…¸ë“œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ ìƒì„±"""
                    if not hasattr(node, 'lineno'):
                        return
                    
                    start = node.lineno - 1
                    end = getattr(node, 'end_lineno', None)
                    if end is None:
                        return
                    
                    # ë…¸ë“œ ìœ í˜•ì— ë”°ë¥¸ ì²˜ë¦¬
                    if isinstance(node, ast.ClassDef):
                        class_name = node.name
                        func_name = None
                        
                        # í´ë˜ìŠ¤ docstring ì¶”ì¶œ
                        docstring = ast.get_docstring(node)
                        
                        # í´ë˜ìŠ¤ ì „ì²´ ì½”ë“œ
                        chunk = '\n'.join(lines[start:end])
                        complexity = calculate_complexity(node)
                        
                        # ë¶€ëª¨ í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ
                        parent_classes = []
                        for base in node.bases:
                            if isinstance(base, ast.Name):
                                parent_classes.append(base.id)
                        
                        # ê°€ë³€ ì²­í¬ í¬ê¸° (ë³µì¡ë„ì— ë”°ë¼ ì¡°ì •)
                        max_tokens = min(512, 128 + complexity * 32)
                        overlap = min(128, 32 + complexity * 8)
                        
                        # í´ë˜ìŠ¤ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
                        if len(enc.encode(chunk)) <= max_tokens:
                            chunks.append((
                                chunk, 0, len(enc.encode(chunk)), 
                                func_name, class_name, start+1, end, 
                                parent_class, complexity, ','.join(parent_classes)
                            ))
                        else:
                            # ì„í¬íŠ¸ + í´ë˜ìŠ¤ ì •ì˜ + docstringì„ ì²« ì²­í¬ì— í¬í•¨
                            class_header = f"{imports_text}\n\n" if imports_text else ""
                            class_def_line = lines[start]
                            if docstring:
                                docstring_lines = docstring.splitlines()
                                class_header += f"{class_def_line}\n    \"\"\"\n    {docstring}\n    \"\"\"\n"
                            else:
                                class_header += f"{class_def_line}\n"
                            
                            chunks.append((
                                class_header, 0, len(enc.encode(class_header)), 
                                func_name, class_name, start+1, start+1+(1 if not docstring else len(docstring.splitlines())+2), 
                                parent_class, complexity, ','.join(parent_classes)
                            ))
                            
                            # ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤ ë³¸ë¬¸ì„ ì²­í‚¹
                            class_body = '\n'.join(lines[start+1:end])
                            for sub_chunk, t_start, t_end in split_by_tokens(class_body, max_tokens=max_tokens, overlap=overlap):
                                chunks.append((
                                    sub_chunk, t_start, t_end, 
                                    func_name, class_name, start+1, end, 
                                    parent_class, complexity, ','.join(parent_classes)
                                ))
                        
                        # í´ë˜ìŠ¤ ë‚´ë¶€ ë©”ì†Œë“œ ì²˜ë¦¬
                        for child in node.body:
                            process_node(child, class_name, None, depth+1)
                    
                    elif isinstance(node, ast.FunctionDef):
                        func_name = node.name
                        
                        # í•¨ìˆ˜ docstring ì¶”ì¶œ
                        docstring = ast.get_docstring(node)
                        
                        # í•¨ìˆ˜ ì „ì²´ ì½”ë“œ
                        chunk = '\n'.join(lines[start:end])
                        complexity = calculate_complexity(node)
                        
                        # ê°€ë³€ ì²­í¬ í¬ê¸° (ë³µì¡ë„ì— ë”°ë¼ ì¡°ì •)
                        max_tokens = min(512, 128 + complexity * 32)
                        overlap = min(128, 32 + complexity * 8)
                        
                        # í•¨ìˆ˜ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
                        if len(enc.encode(chunk)) <= max_tokens:
                            chunks.append((
                                chunk, 0, len(enc.encode(chunk)), 
                                func_name, parent_class, start+1, end, 
                                parent_func, complexity, None
                            ))
                        else:
                            # ì„í¬íŠ¸ + í•¨ìˆ˜ ì •ì˜ + docstringì„ ì²« ì²­í¬ì— í¬í•¨
                            func_header = f"{imports_text}\n\n" if imports_text and not parent_class else ""
                            func_def_line = lines[start]
                            if docstring:
                                docstring_lines = docstring.splitlines()
                                func_header += f"{func_def_line}\n    \"\"\"\n    {docstring}\n    \"\"\"\n"
                            else:
                                func_header += f"{func_def_line}\n"
                            
                            chunks.append((
                                func_header, 0, len(enc.encode(func_header)), 
                                func_name, parent_class, start+1, start+1+(1 if not docstring else len(docstring.splitlines())+2), 
                                parent_func, complexity, None
                            ))
                            
                            # ë‚˜ë¨¸ì§€ í•¨ìˆ˜ ë³¸ë¬¸ì„ ì²­í‚¹
                            func_body = '\n'.join(lines[start+1:end])
                            for sub_chunk, t_start, t_end in split_by_tokens(func_body, max_tokens=max_tokens, overlap=overlap):
                                chunks.append((
                                    sub_chunk, t_start, t_end, 
                                    func_name, parent_class, start+1, end, 
                                    parent_func, complexity, None
                                ))
                        
                        # ì¤‘ì²© í•¨ìˆ˜ ì²˜ë¦¬
                        for child in node.body:
                            process_node(child, parent_class, func_name, depth+1)
                
                # ìµœìƒìœ„ ë…¸ë“œ ì²˜ë¦¬
                for node in tree.body:
                    process_node(node)
                
                # ì²­í¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í† í° ê¸°ë°˜ ì²­í‚¹ ì ìš©
                if not chunks:
                    print(f"[INFO] êµ¬ì¡°ì  ì²­í¬ ì—†ìŒ, í† í° ê¸°ë°˜ ì²­í‚¹ ì ìš©")
                    for chunk, t_start, t_end in split_by_tokens(source_code, max_tokens=256, overlap=64):
                        chunks.append((chunk, t_start, t_end, None, None, 1, len(source_code.splitlines()), None, 0, None))
                
                return chunks
            def chunk_markdown(md_text):
                # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±ì„ ìœ„í•œ ê°œì„ ëœ íŒ¨í„´
                section_pattern = r'(^|\n)(#+\s+.+)($|\n)'  # í—¤ë”
                code_pattern = r'(^|\n)```[\s\S]+?```'  # ì½”ë“œ ë¸”ë¡
                
                # ì„¹ì…˜ ì œëª©ê³¼ ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
                sections = re.finditer(section_pattern, md_text, re.MULTILINE)
                code_blocks = re.finditer(code_pattern, md_text, re.MULTILINE)
                
                # ì„¹ì…˜ê³¼ ì½”ë“œ ë¸”ë¡ì˜ ìœ„ì¹˜ ì •ë³´ ìˆ˜ì§‘
                markers = []
                for section in sections:
                    markers.append((section.start(), section.group(2), 'section'))
                for block in code_blocks:
                    markers.append((block.start(), block.group(0), 'code'))
                
                # ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
                markers.sort(key=lambda x: x[0])
                
                # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
                chunks = []
                last_pos = 0
                for pos, content, marker_type in markers:
                    # ì´ì „ ìœ„ì¹˜ë¶€í„° í˜„ì¬ ë§ˆì»¤ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                    if pos > last_pos:
                        prev_text = md_text[last_pos:pos].strip()
                        if prev_text:
                            if len(enc.encode(prev_text)) > 256:
                                for chunk, t_start, t_end in split_by_tokens(prev_text, max_tokens=256, overlap=64):
                                    chunk_title = "ì¼ë°˜ í…ìŠ¤íŠ¸"
                                    chunks.append((chunk, t_start, t_end, None, chunk_title, None, None, None, 1, None))
                            else:
                                chunk_title = "ì¼ë°˜ í…ìŠ¤íŠ¸"
                                chunks.append((prev_text, 0, len(enc.encode(prev_text)), None, chunk_title, None, None, None, 1, None))
                    
                    # ë§ˆì»¤ ìì²´ ì²˜ë¦¬
                    if marker_type == 'section':
                        # ì„¹ì…˜ ì œëª© ë° ë‹¤ìŒ ë‚´ìš© íŒŒì•…
                        section_title = content
                        next_marker_pos = md_text.find('\n#', pos + len(content)) if pos + len(content) < len(md_text) else -1
                        if next_marker_pos == -1:
                            next_marker_pos = len(md_text)
                        
                        section_content = md_text[pos:next_marker_pos].strip()
                        if len(enc.encode(section_content)) > 256:
                            for chunk, t_start, t_end in split_by_tokens(section_content, max_tokens=256, overlap=64):
                                chunks.append((chunk, t_start, t_end, None, section_title, None, None, None, 2, None))
                        else:
                            chunks.append((section_content, 0, len(enc.encode(section_content)), None, section_title, None, None, None, 2, None))
                        
                        last_pos = next_marker_pos
                    elif marker_type == 'code':
                        code_block = content
                        code_lang = re.search(r'```(\w+)', code_block)
                        code_lang = code_lang.group(1) if code_lang else ''
                        
                        if len(enc.encode(code_block)) > 256:
                            for chunk, t_start, t_end in split_by_tokens(code_block, max_tokens=256, overlap=64):
                                chunks.append((chunk, t_start, t_end, code_lang, "ì½”ë“œ ë¸”ë¡", None, None, None, 3, None))
                        else:
                            chunks.append((code_block, 0, len(enc.encode(code_block)), code_lang, "ì½”ë“œ ë¸”ë¡", None, None, None, 3, None))
                        
                        last_pos = pos + len(code_block)
                
                # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
                if last_pos < len(md_text):
                    remaining_text = md_text[last_pos:].strip()
                    if remaining_text:
                        if len(enc.encode(remaining_text)) > 256:
                            for chunk, t_start, t_end in split_by_tokens(remaining_text, max_tokens=256, overlap=64):
                                chunks.append((chunk, t_start, t_end, None, "ì¼ë°˜ í…ìŠ¤íŠ¸", None, None, None, 1, None))
                        else:
                            chunks.append((remaining_text, 0, len(enc.encode(remaining_text)), None, "ì¼ë°˜ í…ìŠ¤íŠ¸", None, None, None, 1, None))
                
                # ì²­í¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í† í° ê¸°ë°˜ ì²­í‚¹ ì ìš©
                if not chunks:
                    for chunk, t_start, t_end in split_by_tokens(md_text, max_tokens=256, overlap=64):
                        chunks.append((chunk, t_start, t_end, None, "ë§ˆí¬ë‹¤ìš´", None, None, None, 1, None))
                
                return chunks
                
            def chunk_js(source_code):
                """JavaScript ì½”ë“œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì²­í‚¹í•˜ëŠ” í•¨ìˆ˜"""
                # í•¨ìˆ˜/í´ë˜ìŠ¤/ë©”ì†Œë“œ ì •ì˜ íŒ¨í„´
                func_pattern = r'(async\s+)?function\s+(\w+)\s*\([^)]*\)\s*\{'
                arrow_func_pattern = r'(const|let|var)\s+(\w+)\s*=\s*(async\s+)?\([^)]*\)\s*=>'
                class_pattern = r'class\s+(\w+)(\s+extends\s+(\w+))?\s*\{'
                method_pattern = r'(async\s+)?(\w+)\s*\([^)]*\)\s*\{'
                
                lines = source_code.splitlines()
                chunks = []
                
                # ì„í¬íŠ¸/ëª¨ë“ˆ ë¬¸ ì°¾ê¸°
                import_lines = []
                for i, line in enumerate(lines):
                    if re.match(r'^\s*(import|require|export)\b', line):
                        import_lines.append(line)
                
                imports_text = '\n'.join(import_lines)
                
                # ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í•¨ìˆ˜/í´ë˜ìŠ¤ ì°¾ê¸°
                def find_block_end(start_line, opening_char='{', closing_char='}'):
                    """ì¤‘ê´„í˜¸ ì§ì„ ë§ì¶° ë¸”ë¡ ë ë¼ì¸ ì°¾ê¸°"""
                    balance = 0
                    for i in range(start_line, len(lines)):
                        line = lines[i]
                        balance += line.count(opening_char) - line.count(closing_char)
                        if balance <= 0:
                            return i
                    return len(lines) - 1
                
                # í•¨ìˆ˜/í´ë˜ìŠ¤ ì°¾ê¸°
                i = 0
                while i < len(lines):
                    line = lines[i]
                    
                    # í•¨ìˆ˜ ì •ì˜ ì°¾ê¸°
                    func_match = re.search(func_pattern, line)
                    arrow_match = re.search(arrow_func_pattern, line)
                    class_match = re.search(class_pattern, line)
                    
                    if func_match or arrow_match or class_match:
                        start = i
                        
                        if func_match:
                            name = func_match.group(2)
                            is_class = False
                            parent_class = None
                        elif arrow_match:
                            name = arrow_match.group(2)
                            is_class = False
                            parent_class = None
                        else:  # class_match
                            name = class_match.group(1)
                            is_class = True
                            parent_class = class_match.group(3) if class_match.group(2) else None
                        
                        # ë¸”ë¡ ë ì°¾ê¸°
                        end = find_block_end(start)
                        
                        # ì „ì²´ ì½”ë“œ ì²­í¬
                        chunk = '\n'.join(lines[start:end+1])
                        
                        # ë³µì¡ë„ ì¶”ì • (ë¼ì¸ ìˆ˜ + ì¤‘ì²© ë ˆë²¨)
                        complexity = (end - start) // 5 + chunk.count('{') - chunk.count('}')
                        complexity = max(1, complexity)
                        
                        # ê°€ë³€ ì²­í¬ í¬ê¸°
                        max_tokens = min(512, 128 + complexity * 32)
                        overlap = min(128, 32 + complexity * 8)
                        
                        if len(enc.encode(chunk)) <= max_tokens:
                            # ì „ì²´ í•¨ìˆ˜/í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
                            chunks.append((
                                chunk, 
                                0, 
                                len(enc.encode(chunk)), 
                                None if is_class else name, 
                                name if is_class else None, 
                                start+1, 
                                end+1,
                                None,  # parent_func
                                complexity,
                                parent_class if is_class else None
                            ))
                        else:
                            # í—¤ë” (ì„í¬íŠ¸ + í•¨ìˆ˜/í´ë˜ìŠ¤ ì„ ì–¸)
                            header = f"{imports_text}\n\n" if imports_text else ""
                            header += lines[start]
                            
                            chunks.append((
                                header,
                                0,
                                len(enc.encode(header)),
                                None if is_class else name,
                                name if is_class else None,
                                start+1,
                                start+1,
                                None,  # parent_func
                                complexity,
                                parent_class if is_class else None
                            ))
                            
                            # ë³¸ë¬¸ ì²­í‚¹
                            body = '\n'.join(lines[start+1:end+1])
                            for sub_chunk, t_start, t_end in split_by_tokens(body, max_tokens=max_tokens, overlap=overlap):
                                chunks.append((
                                    sub_chunk,
                                    t_start,
                                    t_end,
                                    None if is_class else name,
                                    name if is_class else None,
                                    start+2,  # ë³¸ë¬¸ ì‹œì‘
                                    end+1,
                                    None,  # parent_func
                                    complexity,
                                    parent_class if is_class else None
                                ))
                        
                        # í´ë˜ìŠ¤ ë‚´ë¶€ ë©”ì†Œë“œ ì°¾ê¸° (í´ë˜ìŠ¤ì¸ ê²½ìš°)
                        if is_class:
                            method_start = start + 1
                            while method_start < end:
                                method_line = lines[method_start]
                                method_match = re.search(method_pattern, method_line)
                                
                                if method_match:
                                    method_name = method_match.group(2)
                                    method_end = find_block_end(method_start)
                                    
                                    method_chunk = '\n'.join(lines[method_start:method_end+1])
                                    method_complexity = (method_end - method_start) // 3
                                    
                                    # ë©”ì†Œë“œ ì²­í‚¹
                                    if len(enc.encode(method_chunk)) <= max_tokens // 2:
                                        chunks.append((
                                            method_chunk,
                                            0,
                                            len(enc.encode(method_chunk)),
                                            method_name,
                                            name,  # í´ë˜ìŠ¤ëª…
                                            method_start+1,
                                            method_end+1,
                                            None,
                                            method_complexity,
                                            None
                                        ))
                                    else:
                                        for sub_chunk, t_start, t_end in split_by_tokens(method_chunk, max_tokens=max_tokens//2, overlap=overlap//2):
                                            chunks.append((
                                                sub_chunk,
                                                t_start,
                                                t_end,
                                                method_name,
                                                name,  # í´ë˜ìŠ¤ëª…
                                                method_start+1,
                                                method_end+1,
                                                None,
                                                method_complexity,
                                                None
                                            ))
                                    
                                    method_start = method_end + 1
                                else:
                                    method_start += 1
                        
                        i = end + 1
                    else:
                        i += 1
                
                # ì²­í¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í† í° ê¸°ë°˜ ì²­í‚¹ ì ìš©
                if not chunks:
                    for chunk, t_start, t_end in split_by_tokens(source_code, max_tokens=256, overlap=64):
                        chunks.append((chunk, t_start, t_end, None, None, 1, len(source_code.splitlines()), None, 0, None))
                
                return chunks
            # 1. ì „ì²´ ì²­í¬ ìˆ˜ì§‘
            all_chunks = []
            for file in files:
                content = file['content']
                path = file['path']
                ext = os.path.splitext(path)[1].lower()
                file_name = file.get('file_name')
                file_type = file.get('file_type')
                sha = file.get('sha')
                source_url = file.get('source_url')
                if ext == '.py':
                    chunks = chunk_python_functions(content)
                elif ext == '.md':
                    chunks = chunk_markdown(content)
                elif ext == '.js':
                    chunks = chunk_js(content)
                else:
                    # ì˜¤ë¥˜ ìˆ˜ì •: ì¼ë°˜ íŒŒì¼ì€ split_by_tokensë¡œ ì²˜ë¦¬í•˜ê³  7ê°œ í•„ë“œ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •
                    simple_chunks = split_by_tokens(content, max_tokens=256, overlap=64)
                    chunks = []
                    for chunk_data in simple_chunks:
                        # ì²˜ìŒ 3ê°œ ê°’ì€ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ 4ê°œ í•„ìš”í•œ ê°’ì„ Noneìœ¼ë¡œ ì¶”ê°€
                        chunk, t_start, t_end = chunk_data  # ì—¬ê¸°ì„œ 3ê°œ ê°’ë§Œ ì–¸íŒ¨í‚¹
                        chunks.append((chunk, t_start, t_end, None, None, 1, len(content.splitlines())))
                
                # ì´ ë¶€ë¶„ì´ ì¤‘ìš”: chunksì˜ ëª¨ë“  í•­ëª©ì´ ì •í™•íˆ 7ê°œ ê°’ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
                processed_chunks = []
                for chunk_item in chunks:
                    # ì •í™•íˆ 7ê°œ ê°’ì„ ê°€ì§€ëŠ” íŠœí”Œë¡œ ë³€í™˜
                    if len(chunk_item) == 7:
                        processed_chunks.append(chunk_item)
                    else:
                        # 7ê°œê°€ ì•„ë‹Œ ê²½ìš° í•„ìš”í•œ ë§Œí¼ Noneì„ ì¶”ê°€í•˜ê±°ë‚˜ ì˜ë¼ì„œ 7ê°œë¡œ ë§ì¶¤
                        values = list(chunk_item)[:7]  # ìµœëŒ€ 7ê°œê¹Œì§€ë§Œ ì‚¬ìš©
                        while len(values) < 7:
                            values.append(None)  # 7ê°œê°€ ë  ë•Œê¹Œì§€ None ì¶”ê°€
                        processed_chunks.append(tuple(values))
                
                # ì²˜ë¦¬ëœ chunks ì‚¬ìš©
                for i, (chunk, t_start, t_end, func_name, class_name, start_line, end_line) in enumerate(processed_chunks):
                    # ë””ë²„ê·¸ ì¶œë ¥ ì¶”ê°€í•˜ì—¬ ì‹¤ì œ ê°’ í™•ì¸
                    print(f"[DEBUG] ì²­í¬ ì¶”ê°€: íŒŒì¼={file.get('path')}, ì²­í¬={i}, ê¸¸ì´={len(chunk) if chunk else 0}")
                    all_chunks.append((chunk, file, i, t_start, t_end, func_name, class_name, start_line, end_line))
            # 2. ë¹„ë™ê¸° ì„ë² ë”©+ì—­í• íƒœê¹… í•¨ìˆ˜
            async def embed_and_tag_async(args, client):
                chunk, file, i, t_start, t_end, func_name, class_name, start_line, end_line = args
                # ì„ë² ë”©
                try:
                    emb_resp = await client.embeddings.create(
                        input=chunk,
                        model="text-embedding-3-small"
                    )
                    embedding = emb_resp.data[0].embedding
                except Exception as e:
                    print(f"[WARNING] ì„ë² ë”© ì‹¤íŒ¨: {e}")
                    embedding = [0.0] * 1536
                # ì—­í•  íƒœê¹…
                tag_prompt = f"ì•„ë˜ ì½”ë“œëŠ” ì–´ë–¤ ì—­í• (ê¸°ëŠ¥/ëª©ì )ì„ í•˜ë‚˜ìš”? í•œê¸€ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜.\n\nì½”ë“œ:\n{chunk}"
                try:
                    tag_resp = await client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": tag_prompt}],
                        temperature=0.0,
                        max_tokens=64
                    )
                    role_tag = tag_resp.choices[0].message.content.strip()
                    print(f"[INFO] ì—­í•  íƒœê¹… ê²°ê³¼: íŒŒì¼={file.get('path')}, ì²­í¬={i}, ì—­í• ={role_tag}")
                except Exception as e:
                    print(f"[WARNING] ì—­í•  íƒœê¹… ì‹¤íŒ¨: {e}")
                    role_tag = ''
                return (embedding, role_tag, chunk, file, i, t_start, t_end, func_name, class_name, start_line, end_line)
            # 3. ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰ (max_concurrent=20)
            print(f"[DEBUG] ì„ë² ë”©+ì—­í• íƒœê¹… asyncio ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì²­í¬ ìˆ˜: {len(all_chunks)})")
            semaphore = asyncio.Semaphore(20)
            async def sem_task(args):
                async with semaphore:
                    return await embed_and_tag_async(args, client)
            tasks = [sem_task(args) for args in all_chunks]
            results = await asyncio.gather(*tasks)
            print(f"[DEBUG] ì„ë² ë”©+ì—­í• íƒœê¹… asyncio ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")
            # 4. DB ì €ì¥ (ë™ê¸°)
            for embedding, role_tag, chunk, file, i, t_start, t_end, func_name, class_name, start_line, end_line in results:
                file_name = file.get('file_name')
                file_type = file.get('file_type')
                sha = file.get('sha')
                source_url = file.get('source_url')
                path = file['path']
                # ì²­í¬ íƒ€ì… ê²°ì • (class, method, function, code)  
                chunk_type = "class" if class_name and not func_name else \
                            "method" if class_name and func_name else \
                            "function" if func_name and not class_name else \
                            "code"
                
                # ë³µì¡ë„ ì¶”ì • (ì²­í¬ í¬ê¸° ê¸°ë°˜)
                complexity = 0
                parent_entity = None
                inheritance = None
                
                # ì²­í¬ íŠœí”Œì—ì„œ ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìƒˆ í˜•ì‹ì¸ ê²½ìš°)
                # chunk_data ë³€ìˆ˜ëŠ” ì´ ìŠ¤ì½”í”„ì— ì—†ìœ¼ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                # ëŒ€ì‹  í˜„ì¬ ì–¸íŒ¨í‚¹ëœ ê°’ë“¤ì„ ì‚¬ìš©
                
                metadata = {
                    "path": path or '',
                    "file_name": file_name or '',
                    "file_type": file_type or '',
                    "sha": sha or '',
                    "source_url": source_url or '',
                    "chunk_index": i,
                    "function_name": func_name or '',
                    "class_name": class_name or '',
                    "start_line": start_line if start_line is not None else -1,
                    "end_line": end_line if end_line is not None else -1,
                    "token_start": t_start if t_start is not None else -1,
                    "token_end": t_end if t_end is not None else -1,
                    "role_tag": role_tag,
                    "chunk_type": chunk_type,
                    "complexity": complexity or 1,
                    "parent_entity": parent_entity or '',
                    "inheritance": inheritance or ''
                }
                self.collection.add(
                    ids=[f"{path}_{i}"],
                    embeddings=[embedding],
                    documents=[chunk],
                    metadatas=[safe_meta(metadata)]
                )
                print(f"[INFO] DB ì €ì¥: íŒŒì¼={path}, ì²­í¬={i}, ì—­í• ={role_tag}, ì„ë² ë”© ê¸¸ì´={len(embedding)}")
        # ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° ì‹¤í–‰
        if sys.version_info >= (3, 7):
            asyncio.run(async_process_and_embed(files))
        else:
            raise RuntimeError("Python 3.7 ì´ìƒì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.")